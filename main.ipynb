{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to dataset: https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from itertools import product\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import joblib\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset already split into test, train and val. However, we want to try and do our own data splits. Therefore, merge the data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './archive/'\n",
    "filenames = ['test.txt', 'train.txt', 'val.txt']\n",
    "\n",
    "if not os.path.exists('all.csv'):\n",
    "    # Combine all the files into one csv file\n",
    "    with open('all.csv', 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter=';')\n",
    "        writer.writerow(['text', 'label'])\n",
    "\n",
    "        for fname in filenames:\n",
    "            with open(path + fname) as infile:\n",
    "                for line in infile:\n",
    "                    # Split the line into text and label using the semicolon\n",
    "                    text, label = line.strip().split(';')\n",
    "                    writer.writerow([text, label])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  im feeling rather rotten so im not very ambiti...  sadness\n",
       "1          im updating my blog because i feel shitty  sadness\n",
       "2  i never make her separate from me because i do...  sadness\n",
       "3  i left with my bouquet of red and yellow tulip...      joy\n",
       "4    i was feeling a little vain when i did this one  sadness"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all.csv', delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove stop words and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_stop and len(token) > 1]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words and lemmatize from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('processed.csv'):\n",
    "    # Split the text into tokens\n",
    "    df['text'] = df['text'].apply(process_text)\n",
    "    df.to_csv('processed.csv', index=False)\n",
    "else:\n",
    "    df = pd.read_csv('processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['sadness' 'joy' 'fear' 'anger' 'love' 'surprise']\n",
      "Instances of each category:\n",
      "sadness: 5797\n",
      "joy: 6761\n",
      "fear: 2373\n",
      "anger: 2709\n",
      "love: 1641\n",
      "surprise: 719\n"
     ]
    }
   ],
   "source": [
    "print(f'Categories: {df[\"label\"].unique()}')\n",
    "print('Instances of each category:')\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map labels to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel rotten ambitious right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>update blog feel shitty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>separate not want feel like ashamed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave bouquet red yellow tulip arm feel slight...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel little vain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                        feel rotten ambitious right      0\n",
       "1                            update blog feel shitty      0\n",
       "2                separate not want feel like ashamed      0\n",
       "3  leave bouquet red yellow tulip arm feel slight...      1\n",
       "4                                   feel little vain      0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'fear': 2,\n",
    "    'anger': 3,\n",
    "    'love': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: mapping[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test. Do random oversampling to reduce categorical imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = len(df['label'].unique())\n",
    "# Split the data into training and testing data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared function for evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate best combined score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_score(reports):\n",
    "    total_score = 0\n",
    "    for report in reports:\n",
    "        # Calculate the combined score of precision, recall and f1-score\n",
    "        precision = report['weighted avg']['precision']\n",
    "        recall = report['weighted avg']['recall']\n",
    "        f1_score = report['weighted avg']['f1-score']\n",
    "        accuracy = report['accuracy']\n",
    "        total_score += (2 * precision + recall + f1_score + accuracy) / 5\n",
    "    \n",
    "    return total_score / len(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate model using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kfold(model, train_df):\n",
    "    splits = 5\n",
    "    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=1)\n",
    "    reports = []\n",
    "    for i, (train_i, test_i) in enumerate(skf.split(train_df['text'], train_df['label'])):\n",
    "        text_train, text_validation = train_df['text'][train_i], train_df['text'][test_i]\n",
    "        label_train, label_validation = train_df['label'][train_i], train_df['label'][test_i]\n",
    "        \n",
    "        ros = RandomOverSampler(random_state=1)\n",
    "        text_train, label_train = ros.fit_resample(text_train.values.reshape(-1, 1), label_train)\n",
    "        text_train = text_train.flatten()\n",
    "        \n",
    "        # Create the model and train\n",
    "        model.fit(text_train, label_train)\n",
    "        \n",
    "        # Evaluate the model on validation then test data\n",
    "        report = classification_report(label_validation, model.predict(text_validation), output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "    return combined_score(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build model based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_nb(max_features, ngram_range, alpha, smooth_df, max_df, norm):\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features,\n",
    "                                ngram_range=ngram_range,\n",
    "                                smooth_idf=smooth_df,\n",
    "                                max_df=max_df,\n",
    "                                norm=norm)\n",
    "    classifier = MultinomialNB(alpha=alpha)\n",
    "    return Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8522843711801421\n",
      "Best params: {'max_features': None, 'ngram_range': (1, 2), 'alpha': 0.5, 'smooth_df': True, 'max_df': 0.25, 'norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'tfidf__max_features': [100, 500, 1000, 2000, None],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__alpha': [0.1, 0.5, 1, 2],\n",
    "    'tfidf__smooth_idf': (True, False),\n",
    "    'tfidf__max_df': [0.25, 0.5, 0.75, 1.0],\n",
    "    'tfidf__norm': ('l1', 'l2', None),\n",
    "}\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for max_features, ngram_range, alpha, tfidf__smooth_idf, tfidf__max_df, tfidf__norm in param_combinations:\n",
    "    model = build_model_nb(max_features=max_features, \n",
    "                            ngram_range=ngram_range, \n",
    "                            alpha=alpha, \n",
    "                            smooth_df=tfidf__smooth_idf, \n",
    "                            max_df=tfidf__max_df, \n",
    "                            norm=tfidf__norm)\n",
    "    score = evaluate_kfold(model, train_df)\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = {\n",
    "            'max_features': max_features,\n",
    "            'ngram_range': ngram_range,\n",
    "            'alpha': alpha,\n",
    "            'smooth_df': tfidf__smooth_idf,\n",
    "            'max_df': tfidf__max_df,\n",
    "            'norm': tfidf__norm\n",
    "        }\n",
    "\n",
    "print(f'Best score: {best_score}')\n",
    "print(f'Best params: {best_params}')\n",
    "with open('naive_bayes_best_parameters', 'w') as f:\n",
    "    f.write(str(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89      1146\n",
      "           1       0.92      0.85      0.89      1343\n",
      "           2       0.79      0.81      0.80       475\n",
      "           3       0.84      0.85      0.85       529\n",
      "           4       0.68      0.79      0.73       356\n",
      "           5       0.51      0.78      0.62       151\n",
      "\n",
      "    accuracy                           0.84      4000\n",
      "   macro avg       0.78      0.83      0.79      4000\n",
      "weighted avg       0.86      0.84      0.85      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_train, label_train = train_df['text'], train_df['label']\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "text_train, label_train = ros.fit_resample(text_train.values.reshape(-1, 1), label_train)\n",
    "text_train = text_train.flatten()\n",
    "\n",
    "model = build_model_nb(max_features=None,\n",
    "                    ngram_range=(1, 2),\n",
    "                    alpha=0.5,\n",
    "                    smooth_df=True,\n",
    "                    max_df=0.25,\n",
    "                    norm='l2')\n",
    "\n",
    "# Create the model and train on entire training data\n",
    "model.fit(text_train, label_train)\n",
    "joblib.dump(model, 'naive_bayes.pkl')\n",
    "print(classification_report(test_df['label'], model.predict(test_df['text']), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surprise\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(text):\n",
    "    emotions = ['sadness', 'joy', 'fear', 'anger', 'love', 'surprise']\n",
    "    # Load the model\n",
    "    model = joblib.load('naive_bayes.pkl')\n",
    "    processed_text = process_text(text)\n",
    "    prediction = model.predict([processed_text])\n",
    "    return emotions[prediction[0]]\n",
    "\n",
    "print(naive_bayes('Charles are you fluent in yappanese? damn I didnt know that'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build model based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lr(solver, C, multi_class, tol, max_df, norm):\n",
    "    vectorizer = TfidfVectorizer(max_features=None,\n",
    "                                ngram_range=(1, 2),\n",
    "                                smooth_idf=True,\n",
    "                                max_df=max_df,\n",
    "                                norm=norm,)\n",
    "    classifier = LogisticRegression(solver=solver,\n",
    "                                    C=C,\n",
    "                                    multi_class=multi_class,\n",
    "                                    tol=tol)\n",
    "    return Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 34\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tfidf__max_df, tfidf__norm, clf__solver, clf__C, clf__multi_class, clf__tol \u001b[38;5;129;01min\u001b[39;00m param_combinations:\n\u001b[1;32m     28\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model_lr(max_df\u001b[38;5;241m=\u001b[39mtfidf__max_df,\n\u001b[1;32m     29\u001b[0m                             norm\u001b[38;5;241m=\u001b[39mtfidf__norm,\n\u001b[1;32m     30\u001b[0m                             solver\u001b[38;5;241m=\u001b[39mclf__solver,\n\u001b[1;32m     31\u001b[0m                             C\u001b[38;5;241m=\u001b[39mclf__C,\n\u001b[1;32m     32\u001b[0m                             multi_class\u001b[38;5;241m=\u001b[39mclf__multi_class,\n\u001b[1;32m     33\u001b[0m                             tol\u001b[38;5;241m=\u001b[39mclf__tol)\n\u001b[0;32m---> 34\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n\u001b[1;32m     37\u001b[0m         best_score \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[0;32mIn[109], line 14\u001b[0m, in \u001b[0;36mevaluate_kfold\u001b[0;34m(model, train_df)\u001b[0m\n\u001b[1;32m     11\u001b[0m text_train \u001b[38;5;241m=\u001b[39m text_train\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create the model and train\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluate the model on validation then test data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(label_validation, model\u001b[38;5;241m.\u001b[39mpredict(text_validation), output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1303\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1301\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1303\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    468\u001b[0m     l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[1;32m    469\u001b[0m     args \u001b[38;5;241m=\u001b[39m (X, target, sample_weight, l2_reg_strength, n_threads)\n\u001b[0;32m--> 470\u001b[0m     w0, n_iter_i \u001b[38;5;241m=\u001b[39m \u001b[43m_newton_cg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewton-cholesky\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# The division by sw_sum is a consequence of the rescaling of\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# sample_weight, see comment above.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C \u001b[38;5;241m/\u001b[39m sw_sum\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/utils/optimize.py:194\u001b[0m, in \u001b[0;36m_newton_cg\u001b[0;34m(grad_hess, func, grad, x0, args, tol, maxiter, maxinner, line_search, warn)\u001b[0m\n\u001b[1;32m    190\u001b[0m termcond \u001b[38;5;241m=\u001b[39m eta \u001b[38;5;241m*\u001b[39m maggrad\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Inner loop: solve the Newton update by conjugate gradient, to\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# avoid inverting the Hessian\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m xsupi \u001b[38;5;241m=\u001b[39m \u001b[43m_cg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfhess_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxinner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtermcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m alphak \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line_search:\n",
      "File \u001b[0;32m~/src/ai/intext-emotion-detector/.venv/lib/python3.10/site-packages/sklearn/utils/optimize.py:89\u001b[0m, in \u001b[0;36m_cg\u001b[0;34m(fhess_p, fgrad, maxiter, tol)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(ri)) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tol:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m Ap \u001b[38;5;241m=\u001b[39m \u001b[43mfhess_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsupi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# check curvature\u001b[39;00m\n\u001b[1;32m     91\u001b[0m curv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(psupi, Ap)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'tfidf__max_df': [0.25, 0.5, 0.75, 1.0],\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'clf__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__multi_class': ['ovr', 'multinomial'],\n",
    "    'clf__tol': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "\"\"\" param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__smooth_idf': (True, False),\n",
    "    'tfidf__max_df': [0.25, 0.5, 0.75, 1.0],\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'clf__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__multi_class': ['ovr', 'multinomial'],\n",
    "    'clf__tol': [0.0001, 0.001, 0.01],\n",
    "    'clf__max_iter': [100, 200, 300]\n",
    "} \"\"\"\n",
    "\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for tfidf__max_df, tfidf__norm, clf__solver, clf__C, clf__multi_class, clf__tol in param_combinations:\n",
    "    model = build_model_lr(max_df=tfidf__max_df,\n",
    "                            norm=tfidf__norm,\n",
    "                            solver=clf__solver,\n",
    "                            C=clf__C,\n",
    "                            multi_class=clf__multi_class,\n",
    "                            tol=clf__tol)\n",
    "    score = evaluate_kfold(model, train_df)\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = {\n",
    "            'solver': clf__solver,\n",
    "            'C': clf__C,\n",
    "            'multi_class': clf__multi_class,\n",
    "            'tol': clf__tol,\n",
    "        }\n",
    "        with open('logistic_regression_best_parameters', 'w') as f:\n",
    "            f.write(str(best_params))\n",
    "\n",
    "print(f'Best score: {best_score}')\n",
    "print(f'Best params: {best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      1146\n",
      "           1       0.94      0.89      0.91      1343\n",
      "           2       0.87      0.82      0.85       475\n",
      "           3       0.89      0.88      0.89       529\n",
      "           4       0.74      0.91      0.81       356\n",
      "           5       0.64      0.93      0.76       151\n",
      "\n",
      "    accuracy                           0.89      4000\n",
      "   macro avg       0.84      0.89      0.86      4000\n",
      "weighted avg       0.90      0.89      0.89      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_train, label_train = train_df['text'], train_df['label']\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "text_train, label_train = ros.fit_resample(text_train.values.reshape(-1, 1), label_train)\n",
    "text_train = text_train.flatten()\n",
    "\n",
    "model = build_model_lr(solver='sag',\n",
    "                            C=10,\n",
    "                            multi_class='ovr',\n",
    "                            tol=0.01)\n",
    "\n",
    "# Create the model and train on entire training data\n",
    "model.fit(text_train, label_train)\n",
    "joblib.dump(model, 'logistic_regression.pkl')\n",
    "print(classification_report(test_df['label'], model.predict(test_df['text']), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to run the model on a given piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surprise'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logistic_regression(text):\n",
    "    emotions = ['sadness', 'joy', 'fear', 'anger', 'love', 'surprise']\n",
    "    # Load the model\n",
    "    model = joblib.load('naive_bayes.pkl')\n",
    "    processed_text = process_text(text)\n",
    "    prediction = model.predict([processed_text])\n",
    "    return emotions[prediction[0]]\n",
    "\n",
    "logistic_regression(\"Charles are you fluent in yappanese? damn I didnt know that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
