{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to dataset: https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from itertools import product\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import joblib\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, learning_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset already split into test, train and val. However, we want to try and do our own data splits. Therefore, merge the data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './archive/'\n",
    "filenames = ['test.txt', 'train.txt', 'val.txt']\n",
    "\n",
    "if not os.path.exists('all.csv'):\n",
    "    # Combine all the files into one csv file\n",
    "    with open('all.csv', 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter=';')\n",
    "        writer.writerow(['text', 'label'])\n",
    "\n",
    "        for fname in filenames:\n",
    "            with open(path + fname) as infile:\n",
    "                for line in infile:\n",
    "                    # Split the line into text and label using the semicolon\n",
    "                    text, label = line.strip().split(';')\n",
    "                    writer.writerow([text, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics:\n",
      "Total instances: 20000\n",
      "Unique instances: 19948\n",
      "\n",
      "Categories: ['sadness' 'joy' 'fear' 'anger' 'love' 'surprise']\n",
      "Instances of each category:\n",
      "sadness: 5797 | 28.98%\n",
      "joy: 6761 | 33.80%\n",
      "fear: 2373 | 11.87%\n",
      "anger: 2709 | 13.54%\n",
      "love: 1641 | 8.21%\n",
      "surprise: 719 | 3.60%\n"
     ]
    }
   ],
   "source": [
    "# Get summary statistics\n",
    "print('Summary statistics:')\n",
    "print(f'Total instances: {len(df)}')\n",
    "print(f'Unique instances: {len(df.text.unique())}\\n')\n",
    "print(f'Categories: {df[\"label\"].unique()}')\n",
    "print('Instances of each category:')\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])} | {len(df[df[\"label\"] == label])/len(df):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate entries and show stats after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics after removing duplicates:\n",
      "Total instances: 19948\n",
      "Unique instances: 19948\n",
      "\n",
      "Categories: ['sadness' 'joy' 'fear' 'anger' 'love' 'surprise']\n",
      "Instances of each category:\n",
      "sadness: 5794  | 29.05%\n",
      "joy: 6743  | 33.80%\n",
      "fear: 2366  | 11.86%\n",
      "anger: 2704  | 13.56%\n",
      "love: 1628  | 8.16%\n",
      "surprise: 713  | 3.57%\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "print('Summary statistics after removing duplicates:')\n",
    "print(f'Total instances: {len(df)}')\n",
    "print(f'Unique instances: {len(df.text.unique())}\\n')\n",
    "print(f'Categories: {df[\"label\"].unique()}')\n",
    "print('Instances of each category:')\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])}  | {len(df[df[\"label\"] == label])/len(df):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove stop words and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_stop and len(token) > 1]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words and lemmatize from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('processed.csv'):\n",
    "    # Split the text into tokens\n",
    "    df['text'] = df['text'].apply(process_text)\n",
    "    df.to_csv('processed.csv', index=False)\n",
    "else:\n",
    "    df = pd.read_csv('processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map labels to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'fear': 2,\n",
    "    'anger': 3,\n",
    "    'love': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show data statistics after data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics:\n",
      "Total instances: 19948\n",
      "Unique instances: 19487\n",
      "\n",
      "Categories: [0 1 2 3 4 5]\n",
      "Instances of each category:\n",
      "0: 5794  | 29.05%\n",
      "1: 6743  | 33.80%\n",
      "2: 2366  | 11.86%\n",
      "3: 2704  | 13.56%\n",
      "4: 1628  | 8.16%\n",
      "5: 713  | 3.57%\n"
     ]
    }
   ],
   "source": [
    "print('Summary statistics:')\n",
    "print(f'Total instances: {len(df)}')\n",
    "print(f'Unique instances: {len(df.text.unique())}\\n')\n",
    "print(f'Categories: {df[\"label\"].unique()}')\n",
    "print('Instances of each category:')\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])}  | {len(df[df[\"label\"] == label])/len(df):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = len(df['label'].unique())\n",
    "# Split the data into training and testing data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared function for evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate best combined score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_score(reports):\n",
    "    total_score = 0\n",
    "    for report in reports:\n",
    "        # Calculate the combined score of precision, recall and f1-score\n",
    "        precision = report['weighted avg']['precision']\n",
    "        recall = report['weighted avg']['recall']\n",
    "        f1_score = report['weighted avg']['f1-score']\n",
    "        accuracy = report['accuracy']\n",
    "        total_score += (2 * precision + recall + 2 * f1_score + accuracy) / 6\n",
    "    \n",
    "    return total_score / len(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate model using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kfold(model, train_df):\n",
    "    splits = 5\n",
    "    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=1)\n",
    "    reports = []\n",
    "    for train_i, test_i in skf.split(train_df['text'], train_df['label']):\n",
    "        text_train, text_validation = train_df['text'][train_i], train_df['text'][test_i]\n",
    "        label_train, label_validation = train_df['label'][train_i], train_df['label'][test_i]\n",
    "        \n",
    "        ros = RandomOverSampler(random_state=1)\n",
    "        text_train, label_train = ros.fit_resample(text_train.values.reshape(-1, 1), label_train)\n",
    "        text_train = text_train.flatten()\n",
    "        \n",
    "        # Create the model and train\n",
    "        model.fit(text_train, label_train)\n",
    "        \n",
    "        # Evaluate the model on validation then test data\n",
    "        report = classification_report(label_validation, model.predict(text_validation), output_dict=True, zero_division=0)\n",
    "        reports.append(report)\n",
    "    return combined_score(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build model based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_nb(max_features, ngram_range, alpha, smooth_df, max_df, norm):\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features,\n",
    "                                ngram_range=ngram_range,\n",
    "                                smooth_idf=smooth_df,\n",
    "                                max_df=max_df,\n",
    "                                norm=norm)\n",
    "    classifier = MultinomialNB(alpha=alpha)\n",
    "    return Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8522843711801421\n",
      "Best params: {'max_features': None, 'ngram_range': (1, 2), 'alpha': 0.5, 'smooth_df': True, 'max_df': 0.25, 'norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'tfidf__max_features': [100, 500, 1000, 2000, None],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__alpha': [0.1, 0.5, 1, 2],\n",
    "    'tfidf__smooth_idf': (True, False),\n",
    "    'tfidf__max_df': [0.25, 0.5, 0.75, 1.0],\n",
    "    'tfidf__norm': ('l1', 'l2', None),\n",
    "}\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for max_features, ngram_range, alpha, tfidf__smooth_idf, tfidf__max_df, tfidf__norm in param_combinations:\n",
    "    model = build_model_nb(max_features=max_features, \n",
    "                            ngram_range=ngram_range, \n",
    "                            alpha=alpha, \n",
    "                            smooth_df=tfidf__smooth_idf, \n",
    "                            max_df=tfidf__max_df, \n",
    "                            norm=tfidf__norm)\n",
    "    score = evaluate_kfold(model, train_df)\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = {\n",
    "            'max_features': max_features,\n",
    "            'ngram_range': ngram_range,\n",
    "            'alpha': alpha,\n",
    "            'smooth_df': tfidf__smooth_idf,\n",
    "            'max_df': tfidf__max_df,\n",
    "            'norm': tfidf__norm\n",
    "        }\n",
    "\n",
    "print(f'Best score: {best_score}')\n",
    "print(f'Best params: {best_params}')\n",
    "with open('naive_bayes_best_parameters', 'w') as f:\n",
    "    f.write(str(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89      1146\n",
      "           1       0.92      0.85      0.89      1343\n",
      "           2       0.79      0.81      0.80       475\n",
      "           3       0.84      0.85      0.85       529\n",
      "           4       0.68      0.79      0.73       356\n",
      "           5       0.51      0.78      0.62       151\n",
      "\n",
      "    accuracy                           0.84      4000\n",
      "   macro avg       0.78      0.83      0.79      4000\n",
      "weighted avg       0.86      0.84      0.85      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_train, label_train = train_df['text'], train_df['label']\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "text_train, label_train = ros.fit_resample(text_train.values.reshape(-1, 1), label_train)\n",
    "text_train = text_train.flatten()\n",
    "\n",
    "model = build_model_nb(max_features=None,\n",
    "                    ngram_range=(1, 2),\n",
    "                    alpha=0.5,\n",
    "                    smooth_df=True,\n",
    "                    max_df=0.25,\n",
    "                    norm='l2')\n",
    "\n",
    "# Create the model and train on entire training data\n",
    "model.fit(text_train, label_train)\n",
    "joblib.dump(model, 'naive_bayes.pkl')\n",
    "print(classification_report(test_df['label'], model.predict(test_df['text']), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(text):\n",
    "    emotions = ['sadness', 'joy', 'fear', 'anger', 'love', 'surprise']\n",
    "    # Load the model\n",
    "    model = joblib.load('naive_bayes.pkl')\n",
    "    processed_text = process_text(text)\n",
    "    prediction = model.predict([processed_text])\n",
    "    return emotions[prediction[0]]\n",
    "\n",
    "print(naive_bayes('Charles are you fluent in jappanese?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build model based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lr(solver, C, multi_class, tol, max_df, norm):\n",
    "    vectorizer = TfidfVectorizer(max_features=None,\n",
    "                                ngram_range=(1, 2),\n",
    "                                smooth_idf=True,\n",
    "                                max_df=max_df,\n",
    "                                norm=norm,)\n",
    "    classifier = LogisticRegression(solver=solver,\n",
    "                                    C=C,\n",
    "                                    multi_class=multi_class,\n",
    "                                    tol=tol,\n",
    "                                    max_iter=500)\n",
    "    return Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8921059859048593\n",
      "Best params: {'max_df': 1.0, 'norm': 'l2', 'solver': 'saga', 'C': 10, 'multi_class': 'ovr', 'tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__smooth_idf': (True, False),\n",
    "    'tfidf__max_df': [0.25, 0.5, 1.0],\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'clf__solver': ['newton-cg', 'sag', 'saga'],\n",
    "    'clf__C': [1, 10],\n",
    "    'clf__multi_class': ['ovr', 'multinomial'],\n",
    "    'clf__tol': [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for tfidf__max_df, tfidf__norm, clf__solver, clf__C, clf__multi_class, clf__tol in param_combinations:\n",
    "    model = build_model_lr(max_df=tfidf__max_df,\n",
    "                            norm=tfidf__norm,\n",
    "                            solver=clf__solver,\n",
    "                            C=clf__C,\n",
    "                            multi_class=clf__multi_class,\n",
    "                            tol=clf__tol)\n",
    "    score = evaluate_kfold(model, train_df)\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = {\n",
    "            'max_df': tfidf__max_df,\n",
    "            'norm': tfidf__norm,\n",
    "            'solver': clf__solver,\n",
    "            'C': clf__C,\n",
    "            'multi_class': clf__multi_class,\n",
    "            'tol': clf__tol,\n",
    "        }\n",
    "        with open('logistic_regression_best_parameters', 'w') as f:\n",
    "            f.write(str(best_params))\n",
    "\n",
    "print(f'Best score: {best_score}')\n",
    "print(f'Best params: {best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1146\n",
      "           1       0.93      0.92      0.92      1343\n",
      "           2       0.87      0.84      0.85       475\n",
      "           3       0.92      0.89      0.91       529\n",
      "           4       0.79      0.83      0.81       356\n",
      "           5       0.68      0.82      0.74       151\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.85      0.87      0.86      4000\n",
      "weighted avg       0.90      0.90      0.90      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_train, label_train = train_df['text'], train_df['label']\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "text_train, label_train = ros.fit_resample(text_train.values.reshape(-1, 1), label_train)\n",
    "text_train = text_train.flatten()\n",
    "\n",
    "model = build_model_lr(solver='saga',\n",
    "                        C=10,\n",
    "                        multi_class='ovr',\n",
    "                        tol=0.01,\n",
    "                        max_df=1.0,\n",
    "                        norm='l2')\n",
    "                        \n",
    "# Create the model and train on entire training data\n",
    "model.fit(text_train, label_train)\n",
    "joblib.dump(model, 'logistic_regression.pkl')\n",
    "print(classification_report(test_df['label'], model.predict(test_df['text']), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to run the model on a given piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logistic_regression(text):\n",
    "    emotions = ['sadness', 'joy', 'fear', 'anger', 'love', 'surprise']\n",
    "    # Load the model\n",
    "    model = joblib.load('naive_bayes.pkl')\n",
    "    processed_text = process_text(text)\n",
    "    prediction = model.predict([processed_text])\n",
    "    return emotions[prediction[0]]\n",
    "\n",
    "logistic_regression(\"Charles are you fluent in janpanese?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
